/cm/local/apps/slurm/var/spool/job148456/slurm_script: line 19: cd: too many arguments
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.43s/it]
/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead
  warnings.warn(
/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:832: UserWarning: DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.
  warnings.warn("DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.")
[rank0]: Traceback (most recent call last):
[rank0]:   File "/work/aeslami/VLA/main.py", line 101, in <module>
[rank0]:     manager.fine_tune(
[rank0]:   File "/work/aeslami/VLA/Library/model_manager.py", line 101, in fine_tune
[rank0]:     trainer.train()
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/transformers/trainer.py", line 2095, in _inner_training_loop
[rank0]:     model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/accelerate/accelerator.py", line 1284, in prepare
[rank0]:     result = self._prepare_deepspeed(*args)
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/accelerate/accelerator.py", line 1751, in _prepare_deepspeed
[rank0]:     engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/deepspeed/__init__.py", line 203, in initialize
[rank0]:     engine = DeepSpeedEngine(args=args,
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 355, in __init__
[rank0]:     self._configure_optimizer(optimizer, model_parameters)
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1482, in _configure_optimizer
[rank0]:     self.optimizer = self._configure_fp16_optimizer(basic_optimizer, lp_dtype)
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1700, in _configure_fp16_optimizer
[rank0]:     optimizer = FP16_UnfusedOptimizer(
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/deepspeed/runtime/fp16/unfused_optimizer.py", line 112, in __init__
[rank0]:     self.initialize_optimizer_states()
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/deepspeed/runtime/fp16/unfused_optimizer.py", line 420, in initialize_optimizer_states
[rank0]:     self.optimizer.step()
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
[rank0]:     return wrapped(*args, **kwargs)
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/torch/optim/optimizer.py", line 391, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/torch/optim/adamw.py", line 177, in step
[rank0]:     has_complex = self._init_group(
[rank0]:   File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/torch/optim/adamw.py", line 124, in _init_group
[rank0]:     state["exp_avg"] = torch.zeros_like(
[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 
E1024 12:47:30.436205 140737352876864 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 3372213) of binary: /work/aeslami/VLA/phi_env/bin/python3.10
Traceback (most recent call last):
  File "/work/aeslami/VLA/phi_env/bin/accelerate", line 7, in <module>
    sys.exit(main())
  File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 46, in main
    args.func(args)
  File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1067, in launch_command
    deepspeed_launcher(args)
  File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/accelerate/commands/launch.py", line 771, in deepspeed_launcher
    distrib_run.run(args)
  File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/work/aeslami/VLA/phi_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/work/aeslami/VLA/main.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-24_12:47:30
  host      : node039.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3372213)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
